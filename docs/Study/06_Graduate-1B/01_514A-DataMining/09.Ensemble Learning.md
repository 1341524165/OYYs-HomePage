---
sidebar_position: 9
id: Ensemble Learning
title: Ensemble Learning
tags:
  - Study
  - Graduate
  - Data Mining
---

## Ensemble Learning

### Bias-Variance Tradeoff

| 指标         | 定义                   | 现象       | 原因               |
|--------------|------------------------|------------|--------------------|
| Bias (偏差)  | E[ŷ ‒ y]              | 欠拟合     | 模型过于简单       |
| Variance (方差) | E[(ŷ ‒ E[ŷ])²]      | 过拟合     | 模型对手握复杂     |


### Common Ensemble Methods

#### Stacking (堆叠泛化)
1. Level 0 model: 用原始数据分别训练多个模型
2. Level 1 model: 用Level 0的预测结果作为数据集，再训练一个模型

#### Bootstrapping (自助采样)
对原始数据集进行`有放回的`采样，生成size相同的新数据集

eg. 原始数据集[A, B, C, D, D, G]，采样后的数据集可能包括[A, A, B, D, D, G], [C, D, D, G, G, G] blabla...

#### Bagging (Bootstrap Aggregating)
1. 用Bootstrapping生成多个数据集
2. 用每个数据集训练一个模型
3. 最后将多个模型的预测结果进行`平均`(Regression)或`投票`(Classification)，以综合

-> 降低过拟合风险

#### Random Forest

One motivation behind "ensemble" learning is the idea that different models have different biases when being trained for the same task. For example:

an SVM is a "local" model that focuses on the boundary of two classes
a kNN is a "local" model that focuses of the neighborhood of a test point
a linear regression model is a "global" model that focuses on overall fit
an ANN is a "global" and "local" model that focuses on developing a hierarchical system that pieces together local behavior across a global space
If we just trained a linear regression model on the same data, over and over again to create 10 linear regression models, there wouldn't really be a point in creating an ensemble from that set of models, because we wouldn't expect the models to differ by much. Why, then, does a Random Forest work?

Because each individual decision tree in the Random Forest is trained on different bootstrap sample of data via