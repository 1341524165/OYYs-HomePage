---
sidebar_position: 8
id: SVMs and the Kernel Trick
title: SVMs and the Kernel Trick
tags:
  - Study
  - Graduate
  - Data Mining
---

## SVM

首先，SVM的目标是找到一个超平面（hyperplane），使得它能将数据分成两类，并且距离超平面最近的点到超平面的距离**最大化**。

![beta min](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM_12.png)

举个具体的例子：  
现在我们有一个超平面：
$$
2x_1 + 3x_2 - 6 = 0
$$
即 $ \beta = [2, 3] $, $ b = -6 $

(1) 其中一个支持向量位于：$ x = (4, 1) $

(2) 所以超平面`法向量L2范数`(即**长度**)为：
$$
|| \beta || = \sqrt{2^2 + 3^2} = \sqrt{13}
$$

(3) 该支持向量到超平面的距离为：
$$
d = \frac{2*4 + 3*1 - 6}{\sqrt{13}} = \frac{5}{\sqrt{13}}
$$

(4) $ \alpha $:  
$ \alpha $ 的含义就是 $ d $ 的长度, 即 $ \frac{5}{\sqrt{13}} $

(5) 而向量 $ d $ :
$$
d = \alpha \frac{\beta}{||\beta||} = \frac{5}{\sqrt{13}} * \frac{[2, 3]}{\sqrt{13}} = \frac{5}{13} * [2, 3] = [\frac{10}{13}, \frac{15}{13}]
$$


