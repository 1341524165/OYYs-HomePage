---
sidebar_position: 8
id: SVMs and the Kernel Trick
title: SVMs and the Kernel Trick
tags:
  - Study
  - Graduate
  - Data Mining
---

## SVM

首先，SVM的目标是找到一个超平面（hyperplane），使得它能将数据分成两类，并且距离超平面最近的点到超平面的距离**最大化**。

![beta min](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM_12.png)

_举个具体的例子_：  
现在我们有一个超平面：
$$
2x_1 + 3x_2 - 6 = 0
$$
即 $ \beta = [2, 3] $, $ b = -6 $

(1) 其中一个支持向量位于：$ x = (4, 1) $

(2) 所以超平面`法向量L2范数`(即**长度**)为：
$$
\|\beta\|_2 = \sqrt{2^2 + 3^2} = \sqrt{13}
$$

(3) 该支持向量到超平面的距离为：
$$
d = \frac{2*4 + 3*1 - 6}{\sqrt{13}} = \frac{5}{\sqrt{13}}
$$

(4) $ \alpha $:  
$ \alpha $ 的含义就是 $ d $ 的长度, 即 $ \frac{5}{\sqrt{13}} $

(5) 而向量 $ d $ :
$$
d = \alpha \frac{\beta}{||\beta||} = \frac{5}{\sqrt{13}} * \frac{[2, 3]}{\sqrt{13}} = \frac{5}{13} * [2, 3] = [\frac{10}{13}, \frac{15}{13}]
$$


### Hinge Loss

介绍一个新loss function：Hinge Loss(合页损失)

![Hinge Loss](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM_13.png)

_举个栗子_:  
给定一个样本点 $ x_i $, 它的真实标签是 $ y_i = -1 $, 预测标签是 $ f(x_i) = \beta^T x_i = 0.5 $, 那么它的Hinge Loss为：
$$
h(x, y, \beta) = max(0, 1 - y_i f(x_i)) = max(0, 1 - (-1) * 0.5) = 1.5
$$

你看，如果预测值不是0.5而是1.2，那么Hinge Loss就是0了。  
所以：  
- $ y_i(\beta^T x_i) \ge 1 $, Hinge Loss = 0 -> 分类足够好，不需要优化
- $ y_i(\beta^T x_i) < 1 $, Hinge Loss > 0 -> 损失增大，迫使调整边界，提高分类精度  

所以HL才适用于SVM，因为优化目标就是最大化边界。

### Math Optimization

To balance between maximizing `accuracy` and maximizing `margin`, fit the hyperparameter $ \Lambda $:

SVM loss function:
$$
\min_{\beta} \|\beta\|_2 + \lambda h(x, y, \beta)
$$

**Gradient Descent优化推导**：  

上面式子拆开来：
$$
=> \min_{\beta} \|\beta\|_2 + \lambda \frac{1}{N} \sum_{i=1}^{N} max(0, 1 - y_i \beta^T x_i)
$$

$$
=> \min_{\beta} \frac{1}{N} \sum_{i=1}^{N} [ \ \|\beta\|_2 + \lambda max(0, 1 - y_i \beta^T x_i) \ ]
$$

那最后对 $ \beta $ 求偏导：
$$
\frac{\partial}{\partial \beta} = \frac{1}{N} \sum_{i=1}^{N} 
\begin{cases} 
\beta, & 1 - y_i (\beta^T x_i) \leq 0 \\ 
\beta - \lambda y_i x_i, & \text{otherwise}
\end{cases}
$$


### 厉害的来了!! Dual problem optimization

不管别的，总之SVM问题是strong duality的：
![Dual Problem](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM_14.png)

所以求maximizing `accuracy`和求maximizing `margin`(i.e. minimizing $\|\beta\|_2$)是**等价**的。

#### 拉格朗日乘子

拉格朗日乘子法将`约束问题`转化为`无约束问题`。

目前，我们的约束是：
$$
y_i(\beta^T x_i + b) \ge 1
$$

为了消除这个约束，我们引入`拉格朗日乘子` $ \alpha_i \ge 0 $来代表这个约束的惩罚程度。  
于是我们有拉格朗日函数：
$$
L(\beta, b, \alpha) = \frac{1}{2} \|\beta\|_2^2 - \sum_{i=1}^{N} \alpha_i [ \ y_i(\beta^T x_i + b) - 1 \ ]
$$  


#### 将原问题变换成对偶问题

SVM采用拉格朗日对偶性来求解，即我们需要先对 $ \beta $ 和 $ b $ 求最小，再对 $ \alpha $ 求最大。

1. 对 $ \beta $：
$$
\frac{\partial L}{\partial \beta} = \beta - \sum_{i=1}^{N} \alpha_i y_i x_i = 0
$$

$$
=> \beta = \sum_{i=1}^{N} \alpha_i y_i x_i
$$

2. 对 $ b $：
$$
\frac{\partial L}{\partial b} = - \sum_{i=1}^{N} \alpha_i y_i = 0
$$

$$
=> \sum_{i=1}^{N} \alpha_i y_i = 0
$$

3. 将 $ \beta = \sum_{i=1}^{N} \alpha_i y_i x_i $ 代入 $ L(\beta, b, \alpha) $, 得到对偶问题:

$$
\max_{\alpha} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j x_i^T x_j
$$

