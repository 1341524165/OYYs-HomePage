---
sidebar_position: 10
id: Clustering -- k-Means&Hierarchical Clustering
title: Clustering -- k-Means&Hierarchical Clustering
tags:
  - Study
  - Graduate
  - Data Mining
---


## Clustering: k-Means&Hierarchical Clustering

### Review

我们有两种学习方法to learn from data:
1. **Supervised Learning**: 数据有label - Regression / Classification
2. **Unsupervised Learning**: 无label - `聚类`


### Clustering

#### Intro

Two main principles:
1. Maximize similarity within clusters
2. Minimize similarity between clusters

And clustering can help us to:
1. Discover structure in data
2. Make labeling easier

Validation:
1. 使用一些已知的标签
2. 观察能否得到有意义的新结构


#### Hierarchical Clustering
![Hierarchical Clustering](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM/514DM_25.png)  
Merge的依据是 `距离`。

:::note 一些专业术语
- Agglomerative: 自底向上
- Divisive: 自顶向下
- Dendrogram: 树状图。展示聚类过程，可以从不同高度“剪断”来确定簇数
- Heatmap: 热力图。展示数据的相似性
:::

#### Agglomerative Algorithm
1. 定义距离的函数：
    - Manhattan distance
    - Euclidean distance
    - Hamming distance

2. 定义比较方法：
    - Centroids: 计算每个簇的中心点，比较中心点之间的距离（i.e. 簇均值的距离）
    - Single Linkage: 计算每个簇中`最接近的两个点`之间的距离
    - Complete Linkage: 计算每个簇中`最远的两个点`之间的距离

3. 每个点初始化为一个簇，找到最相近的两个簇，合并它们。重复这个过程，直到所有点都在一个簇中。


#### Divisive Algorithm
1. 定义`Spread`的metric：
    - Average Euclidean distance **from the centroid**
    - Maximum Hamming distance **within the cluster**

2. 选择Spread最大的簇，分裂成两个簇。拆法：
    - 定义两个新的centroid，然后allocate每个点到离它最近的那个centroid

![Pseudo code](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM/514DM_26.png)  
要够闲就读一读伪代码..）


#### !! k-Means Clustering

1. 指定k个簇数
2. 随机选择k个点作为初始centroid
3. E-step: 分配每个点到离它最近的centroid
4. M-step: 计算每个簇的均值，作为新的centroid
5. 重复3-4，直到centroid不再变化

特点：
- 每一步都使得`簇内平方误差（Sum of Squared Errors, SSE）`减少
- 计算复杂度低，适合大数据集，速度快
- 但！！
    - 对初始centroid敏感
    - 只能发现**凸形簇（convex clusters）**以及**大小相似的簇**

为了解决`对初始centroid敏感`的问题，我们一般`multi-starts`：  
每次随机初始化k个centroid，选择SSE最小的结果。  
--> 减少陷入局部最优的风险

:::caution 如何选出最理想的k？
`k`增加的时候，`Spread`会减小。

不过曲线如图：
![Elbow Method](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM/514DM_27.png)

所以我们用`Elbow Method`来选出最理想的k - 即选择**拐点处的k**。这样不浪费性能也保证Spread够小
:::

#### EM Clustering with Gaussian Mixture Models (GMMs)
- 假设数据是由 k 个高斯分布（normal distribution）混合而成
- 每个样本属于每个高斯分布的“概率”不同 -> 软分配（soft clustering）

1. 初始化高斯参数（均值、协方差矩阵、权重）
2. E-step：对每个样本，计算属于每个簇的“概率”
3. M-step：根据这些概率，更新每个簇的参数（相当于重新拟合分布）
4. 重复直到收敛