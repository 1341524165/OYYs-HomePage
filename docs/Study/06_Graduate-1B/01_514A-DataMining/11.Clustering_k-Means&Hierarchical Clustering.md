---
sidebar_position: 10
id: Clustering -- k-Means&Hierarchical Clustering
title: Clustering -- k-Means&Hierarchical Clustering
tags:
  - Study
  - Graduate
  - Data Mining
---


## Clustering: k-Means&Hierarchical Clustering

### Review

我们有两种学习方法to learn from data:
1. **Supervised Learning**: 数据有label - Regression / Classification
2. **Unsupervised Learning**: 无label - `聚类`


### Clustering

#### Intro

Two main principles:
1. Maximize similarity within clusters
2. Minimize similarity between clusters

And clustering can help us to:
1. Discover structure in data
2. Make labeling easier

Validation:
1. 使用一些已知的标签
2. 观察能否得到有意义的新结构


#### Hierarchical Clustering
![Hierarchical Clustering](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM/514DM_25.png)  
Merge的依据是 `距离`。

:::note 一些专业术语
- Agglomerative: 自底向上
- Divisive: 自顶向下
- Dendrogram: 树状图。展示聚类过程，可以从不同高度“剪断”来确定簇数
- Heatmap: 热力图。展示数据的相似性
:::

#### Agglomerative Algorithm
1. 定义距离的函数：
    - Manhattan distance
    - Euclidean distance
    - Hamming distance

2. 定义比较方法：
    - Centroids: 计算每个簇的中心点，比较中心点之间的距离（i.e. 簇均值的距离）
    - Single Linkage: 计算每个簇中`最接近的两个点`之间的距离
    - Complete Linkage: 计算每个簇中`最远的两个点`之间的距离

3. 每个点初始化为一个簇，找到最相近的两个簇，合并它们。重复这个过程，直到所有点都在一个簇中。


#### Divisive Algorithm
1. 定义`Spread`的metric：
    - Average Euclidean distance **from the centroid**
    - Maximum Hamming distance **within the cluster**

2. 选择Spread最大的簇，分裂成两个簇。拆法：
    - 定义两个新的centroid，然后allocate每个点到离它最近的那个centroid

![Pseudo code](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/514DM/514DM_26.png)  
要够闲就读一读伪代码..）


#### !! k-Means Clustering

