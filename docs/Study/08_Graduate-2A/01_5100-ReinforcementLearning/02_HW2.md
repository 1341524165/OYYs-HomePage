---
sidebar_position: 2
id: HW2
title: HW2
tags:
    - Study
    - Graduate
    - Reinforcement Learning
---

## Deep Q-Network (DQN) Experiment Report

---

### 1. Experiment Description

This experiment systematically compares the performance of **7 different DQN variants**:

1. **Basic DQN** - Baseline implementation
2. **Double DQN** - Reduces Q-value overestimation
3. **Dueling DQN** - Separates state value and advantage functions
4. **N-Step (3-step)** - Uses multi-step returns
5. **PER** - Prioritized Experience Replay
6. **N-Step + PER** - Combines N-Step and PER
7. **All-In-One** - Combines all four improvements (Double + Dueling + N-Step + PER)

---

### 2. Experiment Setup

#### 2.1 Environment Configuration

- **Environment**: OpenAI Gymnasium CartPole-v1
- **State Space**: 4-dimensional continuous state (position, velocity, angle, angular velocity)
- **Action Space**: 2 discrete actions (left, right)
- **Reward**: +1 for each time step
- **Termination Condition**: the pole exceeds 12 degrees or the cart moves out of the range
- **Max Score**: 500 points (the maximum episode length of CartPole-v1)

#### 2.2 Training Parameters

```yaml
Training Steps: 50,000
Batch Size: 128
Learning Rate: 0.002
Discount Factor (Î³): 0.99
Target Network Update Frequency: every 3 steps
Epsilon Decay: from 1.0 to 0.05 (12,500 steps)
Hidden Layer Size: 64
Activation Function: ELU
```

---

### 3. Experiment Results

#### 3.1 Convergence Speed Comparison

| Rank | Experiment Name     | Steps to 500 | Relative to Baseline   |
| ---- | ------------------- | ------------ | ---------------------- |
| 1    | **N-Step + PER**    | **4,000**    | **4.5x Acceleration**  |
| 2    | **All-In-One**      | **6,000**    | **3.0x Acceleration**  |
| 3    | **N-Step (3-step)** | **8,000**    | **2.25x Acceleration** |
| 4    | **Double DQN**      | **14,000**   | **1.29x Acceleration** |
| 5    | Basic DQN           | 18,000       | Baseline               |
| 6    | Dueling DQN         | 18,000       | No Improvement         |
| 7    | PER                 | 18,000       | No Improvement         |

We find that:

- `N-Step + PER` combination is the best (4.5x acceleration)
- `N-Step` is the most effective **single** improvement (2.25x acceleration)
- `Double DQN` has a noticeable but small improvement (1.29x acceleration)
- `Dueling DQN` and `PER` alone have **no** obvious advantage in simple tasks

Anyway, all methods eventually reached **Max Score 500.0**, which proves that they all can solve the CartPole task after training.

#### 3.2 Training Curve Analysis

- The training return climbs steadily and converges to 500.
- The `TD loss` shows a high initial variance that diminishes as the policy stabilizes, and **N-Step** has a faster convergence speed.
- The `average Q-values` increase early and stabilize. **Double DQN**'s Q platform is slightly lower and more stable, fitting the description of "overestimating the lower estimate".

Figures for each variant are included below.

![DQN](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/5100DRL/DQN.png)
<sub>DQN: Baseline convergence within ~18k steps.</sub>

![Double DQN](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/5100DRL/Double%20DQN.png)
<sub>Double DQN: Reduced Q overestimation, slightly faster stabilization.</sub>

![Dueling DQN](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/5100DRL/Dueling%20DQN.png)
<sub>Dueling DQN: Similar on CartPole; stronger on tasks with many similar-valued actions.</sub>

![N-Step](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/5100DRL/N-Step.png)
<sub>N-Step (3): Faster convergence via multi-step targets.</sub>

![PER](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/5100DRL/PER.png)
<sub>PER: Mixed effect on this simple task; helps more when dynamics are diverse.</sub>

![NStep + PER](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/5100DRL/N-Step+PER.png)
<sub>N-Step + PER: Best overall speed on this task.</sub>

![All-In-One](https://jcqn.oss-cn-beijing.aliyuncs.com/img_blog/5100DRL/All-in-one.png)
<sub>All-In-One: Strong, close to the best combo.</sub>

---

### 4. Method Analysis

- **N-Step** reduces bias in the target estimate at the expense of slightly higher variance by spreading rewards over several steps, which dramatically speeds up learning.
- **PER** corrects sampling bias using importance sampling weights and gives priority to informative transitions with substantial TD errors; advantages vary depending on the task.
- **Double DQN** reduces overestimation by separating action selection and evaluation; the effect is slight but steady.
- **Dueling DQN** decomposes Q into value and advantage; the benefit is minimal on CartPole.

---

### 5. Experiment Conclusion

- On CartPole, **the combination of N-Step and PER** converges the fastest; the benefit of using **N-Step alone** is also significant.
- **Double DQN** is slightly more stable and has lower overestimation.
- **Dueling DQN** has minimal benefits on this environment.
- All methods eventually reach **Max Score 500**, indicating successful policy learning.

---
