---
sidebar_position: 1
id: Homework1
title: Homework 1
tags:
    - Study
    - Graduate
    - ReinforcementLearning
---

| Yuanjun Feng | 528668 | Homework 1 |
| ------------ | ------ | ---------- |

## Problem 1: MDPs

### question 1

First of all, we have the value iteration formula:

$$
V_{k+1}(s) \,=\, r(s) + \gamma \cdot \max_{a\in\{\text{Run,Fight}\}} \sum_{s'} P(s'\mid s,a)\, V_k(s')\,.
$$

In which,

- $r(s)$: O=+2, D=−1, C=−4
- $\gamma$: 0.9
- Initialization: `J¹(O)=2, J¹(D)=-1, J¹(C)=-4`

Then for $V_2$ from $V_1$, we can calculate the value of each state:  
`V_2(O)`:

- Run:  
  $2 + 0.9\,[0.6\cdot 2 + 0.4\cdot(-1)] = 2 + 0.9\cdot 0.8 = 2.72$
- Fight:  
  $2 + 0.9\,[0.75\cdot 2 + 0.25\cdot(-4)] = 2 + 0.9\cdot 0.5 = 2.45$

We take the max of the two values, so we have:

$$
V_2(O) = 2.72
$$

`V_2(D)`:

- Run:  
  $-1 + 0.9\,[0.4\cdot 2 + 0.3\cdot(-1) + 0.3\cdot(-4)] = -1 + 0.9\cdot (-0.7) = -1.63$
- Fight:  
  $-1 + 0.9\,[0.3\cdot 2 + 0.25\cdot(-1) + 0.45\cdot(-4)] = -1 + 0.9\cdot (-1.45) = -2.305$

We take the max of the two values, so we have:

$$
V_2(D) = -1.63
$$

`V_2(C)`:

- Run:
  $-4 + 0.9\,[0.6\cdot(-1) + 0.4\cdot(-4)] = -4 + 0.9\cdot (-2.2) = -5.98$
- Fight:
  $-4 + 0.9\,[0.2\cdot 2 + 0.8\cdot(-4)] = -4 + 0.9\cdot (-2.8) = -6.52$

We take the max of the two values, so we have:

$$
V_2(C) = -5.98
$$

---

For $V_3$ from $V_2$:

`V_3(O)`:

- Run:
  $2 + 0.9\,[0.6\cdot 2.72 + 0.4\cdot (-1.63)] = 2 + 0.9\cdot 0.98 = 2.882$
- Fight:
  $2 + 0.9\,[0.75\cdot 2.72 + 0.25\cdot (-5.98)] = 2 + 0.9\cdot 0.545 = 2.4905$

We take the max of the two values, so we have:

$$
V_3(O) = 2.882
$$

`V_3(D)`:

- Run:
  $-1 + 0.9\,[0.4\cdot 2.72 + 0.3\cdot (-1.63) + 0.3\cdot (-5.98)] = -1 + 0.9\cdot (-1.195) = -2.0755$
- Fight:
  $-1 + 0.9\,[0.3\cdot 2.72 + 0.25\cdot (-1.63) + 0.45\cdot (-5.98)] = -1 + 0.9\cdot (-2.2825) = -3.05425$

We take the max of the two values, so we have:

$$
V_3(D) = -2.0755
$$

`V_3(C)`:

- Run:
  $-4 + 0.9\,[0.6\cdot (-1.63) + 0.4\cdot (-5.98)] = -4 + 0.9\cdot (-3.370) = -7.033$
- Fight:
  $-4 + 0.9\,[0.2\cdot 2.72 + 0.8\cdot (-5.98)] = -4 + 0.9\cdot (-4.24) = -7.816$

We take the max of the two values, so we have:

$$
V_3(C) = -7.033
$$

---

Finally, we can fill in the table:

| k   | J^k(O) |  J^k(D) | J^k(C) |
| --- | -----: | ------: | -----: |
| 1   |      2 |      -1 |     -4 |
| 2   |   2.72 |   -1.63 |  -5.98 |
| 3   |  2.882 | -2.0755 | -7.033 |

### question 2

For both $k = 2$ and $k = 3$, I would choose **Greedy** policy in state $O$, $D$, and $C$.  
After calculation, the greedy policy with respect to $V_k$ selects `Run` in all three states in both $k = 2$ and $k = 3$.
However, these choices are **not guaranteed** to be the globally optimal policy. Finite iterations yield only a locally optimal policy relative to $V_k$, which would change after **convergence**.
